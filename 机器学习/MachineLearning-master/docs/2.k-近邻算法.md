# 第2章 k-近邻算法
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![k-近邻算法_首页](/images/2.KNN/knn_headPage_xy.png "k-近邻算法首页")

众说周知，电影可以按照题材分类的；而我们却知道每部电影在风格上的确可能和同题材的电影相近。
那么动作片具有哪些共有特征，使得动作片之间非常相似，而与爱情片存在着明显的差别呢？<br/>
例如： 1.打斗次数  2.亲吻次数<br/>
动作片中也会存在接吻镜头，爱情片中也会存在打斗场景，我们不能单纯依靠是否是否存在打斗或者亲吻镜头，爱情片中也会存在打斗场景，动作片中的打斗场景也更频繁，基于此类场景在某部电影中出现的次数可以用来进行电影分类。<br/>
基于电影中出现的亲吻、打斗出现的次数，使用k-近邻算法构造程序，自动划分电影的题材类型。

## k-近邻分类算法

> k-近邻(kNN，k-NearestNeighbor)算法的工作原理

```
    存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。
输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据（最近邻）的分类
标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的来历。通常k是一个不大于20的整数。
    最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。
```

```
    回想上面我们提到的电影分类的例子，使用 k-近邻算法 分类爱情片和动作片。下图显示了6部电影的打斗和接吻镜头数。
    假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？接下来，我们使用 kNN 来解决这个问题。
```


![电影视频案例](/images/2.KNN/knn-1-movie.png "电影视频案例")

```
    现在根据上面我们得到的样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到k个距离最近的电影。
    假定k=3，则三个最靠近的电影依次是， He's Not Really into Dudes 、 Beautiful Woman 和 California Man。
    k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。
```

> k-近邻算法的一般流程

```
收集数据：任何方法
准备数据：距离计算所需要的数值，最好是结构化的数据格式
分析数据：任何方法
训练算法：此步骤不适用于k-近邻算法
测试算法：计算错误率
使用算法：输入样本数据和结构化的输出结果，然后运行k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理
```

> kNN算法伪代码

```
对未知类别属性的数据集中的每个点一次执行一下操作：
(1)计算已知类别数据集中的点与当前点之间的距离
(2)按照距离递增次序排序
(3)选取与当前点距离最小的k个点
(4)确定前k个点所在类别的出现频率
(5)返回前k个点出现频率最高的类别作为当前点的预测分类
```

> k-近邻算法的特点

```
优点：精度高、对异常值不敏感、无数据输入假定
缺点：计算复杂度高、空间复杂度高
适用数据范围：数值型和标称型
```

## 从文本文件中解析和导入数据

> 示例：使用 k-近邻算法改进约会网站的配对效果

```
收集数据：提供文本文件
准备数据：使用Python解析文本文件
分析数据：使用Matlotlib画二维扩散图
训练算法：此步骤不适用于k-近邻算法
测试算法：使用海伦提供的部分数据作为测试样本。
        测试样本和非测试样本的区别在于：
            测试样本是意境完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。
使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。
```

> 将文本记录转换为NumPy的解析程序

```Python
def file2matrix(filename):
    fr = open(filename)
    arrayOLines = fr.readlines()
    numberOfLines = len(arrayOLines)
    returnMat = zeros((numberOfLines,3))
    classLabelVector = []
    index = 0
    for line in arrayOLines:
        line = line.strip()
        listFromLine = line.split('\t')
        returnMat[index,:] = listFromLine[0:3]
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    return returnMat,classLabelVector
```

> NumPy数组和Python数组

```
    本教程以后将大量使用NumPy数组，你既可以直接在Python命令行环境中输入 from numpy import array 将其导入，
也可以通过直接导入所有NumPy库内容来将其导入。由于NumPy库提供的数组操作并不支持Python自带的数组类型，因此在编写代码时要注意不要使用错误的数组类型。
```


## 使用Matplotlib创建扩散图

```Python
import matplotlib
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(datingDataMat[:, 1], datingDataMat[:, 2])
plt.show()
```

## 归一化数值(将数值转化到：0～1之间)

样本3和样本4的距离：
$$\sqrt{(0-67)^2 + (20000-32000)^2 + (1.1-0.1)^2 }$$

我们很容易发现： 上面方程中数字差值最大的属性对计算结果的影响最大。所以我们进行归一化处理是必不可少的。<br>

![约会网站案例](/images/2.KNN/knn-2-date.png "归一化数据")

## 总结

* k-近邻算法其实就是根据空间两个向量距离来判断类别性，关键的是引入k值，保证了一定的稳定性，很明显的缺点就是每次都要与所有样本数据进行对比。
* 文中处理约会数据时，归一化的方法是对于消除影响很十分重要的
* 阅读本章之前建议阅读一下numpy的文档
    * [numpy英文文档](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html "NumPy英文文档")
    * [numpy中文文档](http://old.sebug.net/paper/books/scipydoc/numpy_intro.html "NumPy中文文档")

* * *

* **作者：[羊三](http://www.apache.wiki/display/~xuxin) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**
